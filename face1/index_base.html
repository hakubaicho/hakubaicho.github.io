<!DOCTYPE html>
<html lang="ja">
  <head>
    <!-- 
      * 参考URL *
      表情認識で SNOW 的なアプリ
      https://kkblab.com/make/javascript/face.html

     -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>顔認識</title>
    <style>
      /* video 要素の上に canvas 要素をオーバーレイするための CSS */
      #container {              /* コンテナ用の div について */
        position: relative;     /* 座標指定を相対値指定にする */
      }
      #video {                  /* カメラ映像を流す video について */
        transform: scaleX(-1);  /* 左右反転させる */
      }
      #canvas {                 /* 描画用の canvas について */
        transform: scaleX(-1);  /* 左右反転させる */
        position: absolute;     /* 座標指定を絶対値指定にして */
        left: 0;                /* X座標を0に */
        top: 0;                 /* Y座標を0に */
      }
    </style>
  </head>
 
<body>
  <div id="container">  <!-- video の上に canvas をオーバーレイするための div 要素 -->
    <video id="video" width="400" height="300" autoplay></video>  <!-- カメラ映像を流す video -->
    <canvas id="canvas" width="400" height="300"></canvas>        <!-- 重ねて描画する canvas -->
  </div>
  <div id="dat"></div>  <!-- データ表示用 div 要素 -->
 
  <!-- clmtrackr 関連ファイルの読み込み -->
  <script src="clmtrackr.js"></script>          <!-- clmtrackr のメインライブラリの読み込み -->
  <script src="model_pca_20_svm.js"></script>   <!-- 顔モデル（※）の読み込み -->
  
  <script>
    // もろもろの準備
    var video = document.getElementById("video");           // video 要素を取得
    var canvas = document.getElementById("canvas");         // canvas 要素の取得
    var context = canvas.getContext("2d");                  // canvas の context の取得
    
    // getUserMedia によるカメラ映像の取得
    var media = navigator.mediaDevices.getUserMedia({       // メディアデバイスを取得
      video: {facingMode: "user"},                          // カメラの映像を使う（スマホならインカメラ）
      audio: false                                          // マイクの音声は使わない
    });
    media.then((stream) => {                                // メディアデバイスが取得できたら
      video.srcObject = stream;                             // video 要素にストリームを渡す
    });
    
    // clmtrackr の開始
    var tracker = new clm.tracker();  // tracker オブジェクトを作成
    tracker.init(pModel);             // tracker を所定のフェイスモデル（※）で初期化
    tracker.start(video);             // video 要素内でフェイストラッキング開始
    
    // 描画ループ
    function drawLoop() {
      requestAnimationFrame(drawLoop);                      // drawLoop 関数を繰り返し実行
      var positions = tracker.getCurrentPosition();         // 顔部品の現在位置の取得
      showData(positions);                                  // データの表示
      context.clearRect(0, 0, canvas.width, canvas.height); // canvas をクリア
      tracker.draw(canvas);                                 // canvas にトラッキング結果を描画
    }
    drawLoop();                                             // drawLoop 関数をトリガー
    
    // 顔部品（特徴点）の位置データを表示する showData 関数
    function showData(pos) {
      var str = "";                                         // データの文字列を入れる変数
      for(var i = 0; i < pos.length; i++) {                 // 全ての特徴点（71個）について
        str += "特徴点" + i + ": ("
            + Math.round(pos[i][0]) + ", "                 // X座標（四捨五入して整数に）
            + Math.round(pos[i][1]) + ")<br>";             // Y座標（四捨五入して整数に）
      }
      var dat = document.getElementById("dat");             // データ表示用div要素の取得
      dat.innerHTML = str;                                  // データ文字列の表示
    }
    </script>
  </body>
</html>